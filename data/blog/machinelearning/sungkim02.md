---
title: "ğŸ“PyTorch Zero to All - Lecture02 Overview"
date: 2021-02-21 05:15:00 +0900
tags: ['MACHINE-LEARNING', 'PYTHON']
draft: false
summary: "Sung Kim's PyTorch Lecture 02 review, ì„±í‚´ êµìˆ˜ë‹˜ì˜ ë¨¸ì‹ ëŸ¬ë‹ ê°•ì˜ ë¦¬ë·°"
cover: ""
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/l-Fe9Ekxxj4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

# Model Design
ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª¨ë¸ì—ëŠ” 3ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤.
- Supervised Learning(ì§€ë„í•™ìŠµ)
- Unsupervised Learning(ë¹„ì§€ë„í•™ìŠµ)
- Reinforcement Learning(ê°•í™”í•™ìŠµ)

êµìˆ˜ë‹˜ì˜ ê°•ì˜ì—ì„œ ì‚¬ìš©ë  ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¢…ë¥˜ëŠ” Supervised Learning(ì§€ë„í•™ìŠµ) íƒ€ì…ì´ë‹¤.

ê°•ì˜ ì˜ìƒì—ì„œëŠ” ì‹œê°„ì— ë”°ë¥¸ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ì˜ˆì œì‚¼ì•„ ì„¤ê³„ë¥¼ ì§„í–‰í•œë‹¤.

ìš°ì„ , ì§€ë„í•™ìŠµì„ í•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ë°ì´í„°ë“¤ì˜ ë¶„ë¥˜ê°€ í•„ìš”í•˜ë‹¤. í¬ê²Œ **Training dataset** ê³¼ **Test dataset**ìœ¼ë¡œ ë‚˜ë‰œë‹¤. ì˜ˆì œ ì† ë‘ ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

| Hours(x) | Points(y) | |
| --- | --- | --- |
| 1 | 2 | Train |
| 2 | 4 | Train |
| 3 | 6 | Train |
| 4 | ? | Test |

ë§ˆì§€ë§‰ "?"ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì„¤ê³„í•˜ëŠ” ê²ƒì´ í˜„ì¬ì˜ ëª©í‘œì´ë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì •ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì€ ì„ í˜•(Linear)ëª¨ë¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê¸°ì´ˆì ì¸ ì„ í˜• ë°©ì •ì‹ì„ í† ëŒ€ë¡œ ë‚˜ì¤‘ì— ì¶”ê°€ë  ê°€ì¤‘ì¹˜(weight)ê°œë…ê³¼ ë°”ì´ì–´ìŠ¤(bias) ê°œë…ê¹Œì§€ í™œìš©í•˜ì—¬ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\hat{y} = x * w + b
$$

$$
x\rightarrow Linear \rightarrow \hat{y}
$$

ë‘ ì‹ì¤‘, ìœ„ì˜ ì‹ $\hat{y} = x * w + b$ ì—ì„œ bëŠ” ë”ìš± ê°„ë‹¨í•œ ì‹ì˜ í‘œí˜„ì„ ìœ„í•´ ìƒëµí•˜ê³  ê·¸ë˜í”„ë¥¼ ì‘ì„±í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

ìœ„ì˜ ê·¸ë˜í”„ì˜ ì„ ì€ ì…ë ¥ì— ëŒ€í•œ ì •ë‹µì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” **True Line**ì´ë‹¤. ì§€ê¸ˆë¶€í„° ìš°ë¦¬ê°€ ì„¤ê³„í•œ ëª¨ë¸ì€ í•´ë‹¹ True Lineì— ê°€ì¥ ê·¼ì ‘í•  ìˆ˜ ìˆëŠ” $w$ê°’ì„ random guessingí•œë‹¤. ë¬´ì‘ìœ„ ì˜ˆì¸¡ì´ê¸° ë•Œë¬¸ì—, True Lineì—ì„œëŠ” ë¹—ê»´ê°„ ë‹¤ì–‘í•œ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì´ ë“±ì¥í•œë‹¤. ê·¸ë˜í”„ ìœ„ì— í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

ì§€ê¸ˆ ë¶€í„° ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€, ì˜ˆì¸¡ëœ ë‹¤ì–‘í•œ ê·¸ë˜í”„ë“¤ì´ ìµœëŒ€í•œ True Lineì— **ê°€ê¹ë„ë¡** í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ê·¸ë˜í”„ ë³„ë¡œ True Lineê³¼ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ì—¬ ì¤„ì´ëŠ” ê²ƒì´ë‹¤. True Lineì„ $y$, ìš°ë¦¬ê°€ ë§Œë“  ê·¸ë˜í”„ë¥¼ $\hat{y}$ë¡œ ë³´ì•˜ì„ ë•Œ, ë‘ ê·¸ë˜í”„ì˜ ê±°ë¦¬ëŠ” $y - \hat{y}$ ì´ë‹¤. ìš°ë¦¬ì—ê²ŒëŠ” ë¶€í˜¸ì˜ ì˜ë¯¸ê°€ í•„ìš”ì—†ê³  ê±°ë¦¬ì˜ í¬ê¸°ê°€ í•„ìš”í•˜ê¸°ì— ì œê³±ì„ ì”Œì–´ì¤€ë‹¤. ì´ë ‡ê²Œ ë‚˜ì˜¨ ê°’ì„ **error(ì˜¤ë¥˜)** ë¼ê³  ë¶€ë¥´ê³ , ì´ëŸ¬í•œ ê°’ì„ ì–»ëŠ” í•¨ìˆ˜ë¥¼ **Loss Function(ì†ì‹¤í•¨ìˆ˜)** ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ëŸ¬í•œ ì˜¤ë¥˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ìµœì„ ì˜ ê°’ì„ ì‚°ì¶œí•´ ë‚¼ í•´ë²•ì´ë‹¤.ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
loss = (\hat{y} - y)^2 = (x * w - y) ^ 2
$$

ë‹¤ìŒì€ $w$ ê°’ì˜ ë³€í™”ì— ë”°ë¥¸ ì†ì‹¤ì˜ ë³€í™”ë¥¼ ì •ë¦¬í•œ í‘œì´ë‹¤.

MSE = Mean Square Error, ì†ì‹¤ ì œê³± í‰ê·   
    => $loss =$ $1\over N$ $\sum_{n=1}^N (\hat{y_n} - y_n)^2$

|Hours, $x$|Loss($w$ = 0)|Loss($w$ = 1)|Loss($w$ = 2)|Loss($w$ = 3)|Loss($w$ = 4)|
|---|---|---|---|---|---|
|1|4|1|0|1|4|
|2|16|4|0|4|16|
|3|36|9|0|9|36|
||MSE = 56/3 = 18.7|MSE = 14/3 = 4.7|MSE = 0|MSE = 14/3 = 4.7|MSE = 56/3 = 18.7|

ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

# Let's code
sungkim êµìˆ˜ë‹˜ì˜ ëª¨ë“  ì½”ë“œëŠ” í•´ë‹¹ [git-hub](https://github.com/hunkim/PyTorchZeroToAll/blob/master/01_basics.py)ì— ì •ë¦¬ ë˜ì–´ ì˜¬ë¼ì™€ ìˆë‹¤. ê·¸ ì¤‘ ê°€ë…ì„±ê³¼ ì •ë¦¬ë¥¼ ìœ„í•´ ì¼ë¶„ì”©ë§Œ ë°œì·Œí•˜ì—¬ ì„¤ëª…ê³¼ í•¨ê»˜ ì½”ë“œë¥¼ ì •ë¦¬í•´ ë³´ê² ë‹¤.

ìš°ì„ , ì˜ˆì œë¡œ ì‚¼ì€ ì‹œê°„ì— ë”°ë¥¸ ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ ì—°ì‚°ì„ ìœ„í•´ í•„ìš”í•œ ê¸°ì´ˆì ì¸ íŒ¨í‚¤ì§€ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê³  ìš°ë¦¬ê°€ ì„¤ì •í•œ ë°ì´í„° ê°’ì„ ë„£ì–´ì¤€ë‹¤.
```python
import numpy as np
import matplotlib.pyplot as plt

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
```

ë‹¤ìŒìœ¼ë¡œ $\hat{y} = x * w$ì— í•´ë‹¹í•˜ëŠ” íŒŒíŠ¸ì¸ ëª¨ë¸ ì„¤ê³„ í•¨ìˆ˜ë¥¼ ì„¤ì •í•˜ì.
```python
w = 1.0

# our model for the forward pass
def forward(x):
    return x * w
```

ì†ì‹¤í•¨ìˆ˜$loss = (\hat{y} - y)^2$ë„ ì½”ë“œë¡œ ì‘ì„±í•˜ì
```python
# Loss function
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) * (y_pred - y)
```

$w$ì— ë”°ë¥¸ ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•œ ë°˜ë³µë¬¸ ë¶€ë¶„ë„ ìˆë‹¤.
```python
for w in np.arange(0.0, 4.1, 0.1):
    # Print the weights and initialize the lost
    print("w=", w)
    l_sum = 0

    for x_val, y_val in zip(x_data, y_data):
        # For each input and output, calculate y_hat
        # Compute the total loss and add to the total error
        y_pred_val = forward(x_val)
        l = loss(x_val, y_val)
        l_sum += l
        print("\t", x_val, y_val, y_pred_val, l)
    # Now compute the Mean squared error (mse) of each
    # Aggregate the weight/mse from this run
    print("MSE=", l_sum / len(x_data))
```

$w$ì— ë”°ë¥¸ ì†ì‹¤ ë³€í™”ë¥¼ ì˜ í™•ì¸í•˜ê¸°ìœ„í•´ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì„œ í™•ì¸í•  ìˆ˜ë„ ìˆë‹¤. ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì½”ë“œë¥¼ ì¶”ê°€í•´ì£¼ë©´ ê°€ëŠ¥í•˜ë‹¤.
```python
    w_list.append(w)
    mse_list.append(l_sum / len(x_data))

# Plot it all
plt.plot(w_list, mse_list)
plt.ylabel('Loss')
plt.xlabel('w')
plt.show()
```
ìœ„ ì½”ë“œì—ì„œ ë‘ì¤„ì€, ì´ì „ ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•œ ë°˜ë³µë¬¸ ì•ˆì— ë„£ì–´ì£¼ê¸°ë§Œ í•˜ë©´ ì™¸ë¶€ì˜ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì„œ í•œë²ˆì— ê°€ê³µí•˜ì—¬ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤. ê·¸ë ¤ì§„ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

---
ì°¸ê³ ìë£Œ : [Youtube Link](https://youtu.be/l-Fe9Ekxxj4)

[pytorchzerotoall_github](https://github.com/hunkim/PyTorchZeroToAll/blob/master/01_basics.py)